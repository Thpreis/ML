\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}{Gradient descent}}{36}{section.2.5}}
\newlabel{sec:gd}{{2.5}{36}{Gradient descent}{section.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}{Simple gradient descent}}{36}{subsection.2.5.1}}
\newlabel{eq:gdsimple}{{2.29}{36}{Simple gradient descent}{equation.2.5.29}{}}
\mph@setcol{ii:36}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}{Modified gradient descent}}{37}{subsection.2.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2.1}{Stochastic gradient descent (SGD) with mini-batches}}{37}{subsubsection.2.5.2.1}}
\newlabel{subsubsec:gdSGD}{{2.5.2.1}{37}{Stochastic gradient descent (SGD) with mini-batches}{subsubsection.2.5.2.1}{}}
\newlabel{eq:gdstochastic}{{2.30}{37}{Stochastic gradient descent (SGD) with mini-batches}{equation.2.5.30}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2.2}{Algorithm gradient descent with momentum}}{37}{subsubsection.2.5.2.2}}
\def\mph@nr{5}
\newlabel{eq:gdmomentum}{{2.31}{37}{Algorithm gradient descent with momentum}{equation.2.5.31}{}}
\newlabel{eq:gdNAG}{{2.32}{37}{Algorithm gradient descent with momentum}{equation.2.5.32}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2.3}{Methods that use the second moment of the gradient}}{37}{subsubsection.2.5.2.3}}
\mph@setcol{ii:37}{\mph@nr}
\newlabel{eq:gdRMSprop}{{2.33}{38}{Methods that use the second moment of the gradient}{equation.2.5.33}{}}
\newlabel{eq:gdADAM}{{2.34}{38}{Methods that use the second moment of the gradient}{equation.2.5.34}{}}
\mph@setcol{ii:38}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}{Practical tips for using GD}}{39}{subsection.2.5.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.6}{Linear regression}}{39}{section.2.6}}
\newlabel{sec:linearRegression}{{2.6}{39}{Linear regression}{section.2.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}{Least-square regressions -frequentist}}{39}{subsection.2.6.1}}
\mph@setcol{ii:39}{\mph@nr}
\newlabel{eq:lregOLS}{{2.35}{40}{Least-square regressions -frequentist}{equation.2.6.35}{}}
\newlabel{eq:lregOLSsolution}{{2.36}{40}{Least-square regressions -frequentist}{equation.2.6.36}{}}
\newlabel{eq:lregOLbestfit}{{2.37}{40}{Least-square regressions -frequentist}{equation.2.6.37}{}}
\mph@setcol{ii:40}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}{Regularized Least-square regressions-frequentist}}{41}{subsection.2.6.2}}
\mph@setcol{ii:41}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2.1}{Ridge-Regression}}{42}{subsubsection.2.6.2.1}}
\newlabel{eq:lregRidge}{{2.40}{42}{Ridge-Regression}{equation.2.6.40}{}}
\newlabel{eq:lregRidgeSol}{{2.41}{42}{Ridge-Regression}{equation.2.6.41}{}}
\mph@setcol{ii:42}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2.2}{LASSO and Sparse Regression}}{43}{subsubsection.2.6.2.2}}
\newlabel{eq:lregLASSO}{{2.45}{43}{LASSO and Sparse Regression}{equation.2.6.45}{}}
\newlabel{eq:lregLASSOSol}{{2.46}{43}{LASSO and Sparse Regression}{equation.2.6.46}{}}
\mph@setcol{ii:43}{\mph@nr}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax }}{44}{figure.caption.4}}
\newlabel{fig:lassovsridge}{{2.3}{44}{\relax }{figure.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2.3}{A note on LASSO and Ridge}}{44}{subsubsection.2.6.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2.4}{Another note being a general comment on regularization}}{44}{subsubsection.2.6.2.4}}
\mph@setcol{ii:44}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2.5}{A general perspective on regularizers}}{45}{subsubsection.2.6.2.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}{Bayesian formulation of linear regression}}{45}{subsection.2.6.3}}
\newlabel{subsec:lregBayesian}{{2.6.3}{45}{Bayesian formulation of linear regression}{subsection.2.6.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3.1}{Regularization }}{45}{subsubsection.2.6.3.1}}
\@writefile{tdo}{\defcounter {refsection}{0}\relax }\@writefile{tdo}{\contentsline {todo}{todo ?}{45}{section*.5}}
\def\mph@nr{6}
\mph@setcol{ii:45}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}{Outlook from linear regression}}{46}{subsection.2.6.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.7}{Logistic Regression}}{46}{section.2.7}}
\newlabel{sec:logisticRegression}{{2.7}{46}{Logistic Regression}{section.2.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}{Mathematical set-up}}{46}{subsection.2.7.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1.1}{Binary classification}}{46}{subsubsection.2.7.1.1}}
\newlabel{subsubsec:classBinary}{{2.7.1.1}{46}{Binary classification}{subsubsection.2.7.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1.2}{Multi-class classification}}{46}{subsubsection.2.7.1.2}}
\newlabel{subsubsec:classMultiClass}{{2.7.1.2}{46}{Multi-class classification}{subsubsection.2.7.1.2}{}}
\mph@setcol{ii:46}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}{Classifiers}}{47}{subsection.2.7.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2.1}{On threshold function functions}}{47}{subsubsection.2.7.2.1}}
\newlabel{subsubsec:thresholdfct}{{2.7.2.1}{47}{On threshold function functions}{subsubsection.2.7.2.1}{}}
\newlabel{eq:logRegsign}{{2.47}{47}{On threshold function functions}{equation.2.7.47}{}}
\newlabel{eq:logRegSigmoid}{{2.48}{47}{On threshold function functions}{equation.2.7.48}{}}
\@writefile{tdo}{\defcounter {refsection}{0}\relax }\@writefile{tdo}{\contentsline {todo}{Put definition here}{47}{section*.6}}
\def\mph@nr{7}
\newlabel{eq:logRegHyperbolicTangent}{{2.49}{47}{On threshold function functions}{equation.2.7.49}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}{Perceptron Learning Algorithm (PLA)}}{47}{subsection.2.7.3}}
\mph@setcol{ii:47}{\mph@nr}
\newlabel{eq:logRegPerceptron}{{2.50}{48}{Perceptron Learning Algorithm (PLA)}{equation.2.7.50}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.3.1}{The algorithm}}{48}{subsubsection.2.7.3.1}}
\mph@setcol{ii:48}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}{Definition of logistic regression - Bayesian}}{49}{subsection.2.7.4}}
\newlabel{eq:logregCrossEntropy}{{2.53}{49}{Definition of logistic regression - Bayesian}{equation.2.7.53}{}}
\newlabel{eq:logregCrossEntropyMinimized}{{2.54}{49}{Definition of logistic regression - Bayesian}{equation.2.7.54}{}}
\mph@setcol{ii:49}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.4.1}{On the 2d Ising example}}{50}{subsubsection.2.7.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.4.2}{SUSY }}{50}{subsubsection.2.7.4.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}{SoftMax regression}}{50}{subsection.2.7.5}}
\newlabel{subsec:logregSoftMax}{{2.7.5}{50}{SoftMax regression}{subsection.2.7.5}{}}
\mph@setcol{ii:50}{\mph@nr}
\newlabel{eq:logregSoftMaxfct}{{2.56}{51}{SoftMax regression}{equation.2.7.56}{}}
\newlabel{eq:logregSoftMaxcostFct}{{2.57}{51}{SoftMax regression}{equation.2.7.57}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.8}{Ensemble Methods - On combining models}}{51}{section.2.8}}
\newlabel{sec:ensembles}{{2.8}{51}{Ensemble Methods - On combining models}{section.2.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}{Introduction}}{51}{subsection.2.8.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1.1}{Motivation}}{51}{subsubsection.2.8.1.1}}
\newlabel{subsubsec:ensemblesMotivation}{{2.8.1.1}{51}{Motivation}{subsubsection.2.8.1.1}{}}
\mph@setcol{ii:51}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1.2}{Benefits of Ensemble Methods before diving in}}{52}{subsubsection.2.8.1.2}}
\mph@setcol{ii:52}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}{Aggregate predictor methods - Bagging and Boosting}}{53}{subsection.2.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.1}{Bagging}}{53}{subsubsection.2.8.2.1}}
\newlabel{subsubsec:ensemblesBagging}{{2.8.2.1}{53}{Bagging}{subsubsection.2.8.2.1}{}}
\newlabel{eq:ensemblesBaggingaggregatePredictorcontinuous}{{2.59}{53}{Bagging}{equation.2.8.59}{}}
\newlabel{eq:ensemblesBaggingaggregatePredictorclassification}{{2.60}{53}{Bagging}{equation.2.8.60}{}}
\mph@setcol{ii:53}{\mph@nr}
\newlabel{eq:ensemblesBaggingBSaggregatePredictorcontinuous}{{2.61}{54}{Bagging}{equation.2.8.61}{}}
\newlabel{eq:ensemblesBaggingBSaggregatePredictorclassification}{{2.62}{54}{Bagging}{equation.2.8.62}{}}
\mph@setcol{ii:54}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.2}{Boosting}}{55}{subsubsection.2.8.2.2}}
\newlabel{subsubsec:ensemblesBoosting}{{2.8.2.2}{55}{Boosting}{subsubsection.2.8.2.2}{}}
\newlabel{eq:ensemblesBoostingAggregateClassifier}{{2.63}{55}{Boosting}{equation.2.8.63}{}}
\mph@setcol{ii:55}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.3}{Random Forests}}{56}{subsection.2.8.3}}
\newlabel{subsec:ensemblesRandomForest}{{2.8.3}{56}{Random Forests}{subsection.2.8.3}{}}
\mph@setcol{ii:56}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.4}{Gradient Boosted Trees and XGBoost}}{57}{subsection.2.8.4}}
\newlabel{subsec:ensemblesGBoostedTreesandXGBoost}{{2.8.4}{57}{Gradient Boosted Trees and XGBoost}{subsection.2.8.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.4.1}{XGBoost}}{57}{subsubsection.2.8.4.1}}
\newlabel{subsubsec:ensemblesXGBoost}{{2.8.4.1}{57}{XGBoost}{subsubsection.2.8.4.1}{}}
\mph@setcol{ii:57}{\mph@nr}
\newlabel{eq:ensemblesXGBoostCostfct}{{2.65}{58}{XGBoost}{equation.2.8.65}{}}
\mph@setcol{ii:58}{\mph@nr}
\mph@setcol{ii:59}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.9}{An Introduction to Feed-Forward Deep Neural Networks (DNNS)}}{60}{section.2.9}}
\newlabel{sec:dnn}{{2.9}{60}{An Introduction to Feed-Forward Deep Neural Networks (DNNS)}{section.2.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.1}{Neural Network Basics}}{60}{subsection.2.9.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1.1}{The basic building block: neurons}}{60}{subsubsection.2.9.1.1}}
\newlabel{subsubsec:dnnNeurons}{{2.9.1.1}{60}{The basic building block: neurons}{subsubsection.2.9.1.1}{}}
\mph@setcol{ii:60}{\mph@nr}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \itshape  A) Neurons consist of a linear transformation that weights the importance of various inputs, followed by a non-linear activation function. B) Network architecture.\relax }}{61}{figure.caption.7}}
\newlabel{fig:neuron}{{2.4}{61}{\itshape A) Neurons consist of a linear transformation that weights the importance of various inputs, followed by a non-linear activation function. B) Network architecture.\relax }{figure.caption.7}{}}
\mph@setcol{ii:61}{\mph@nr}
\newlabel{eq:dnnInputOutputfct}{{2.71}{62}{The basic building block: neurons}{equation.2.9.71}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1.2}{Layering neurons to build deep networks: network architecture}}{62}{subsubsection.2.9.1.2}}
\newlabel{subsubsec:dnnNetworkArchitecture}{{2.9.1.2}{62}{Layering neurons to build deep networks: network architecture}{subsubsection.2.9.1.2}{}}
\mph@setcol{ii:62}{\mph@nr}
\mph@setcol{ii:63}{\mph@nr}
\mph@setcol{ii:64}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1.3}{Further on network architecture: How many layers is adequate for a problem ?}}{65}{subsubsection.2.9.1.3}}
\newlabel{subsubsec:dnnBestPracticeArchitecture}{{2.9.1.3}{65}{Further on network architecture: How many layers is adequate for a problem ?}{subsubsection.2.9.1.3}{}}
\mph@setcol{ii:65}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.2}{Training deep networks}}{66}{subsection.2.9.2}}
\newlabel{subsec:dnnTraining}{{2.9.2}{66}{Training deep networks}{subsection.2.9.2}{}}
\newlabel{eq:dnnCostSquaredError}{{2.72}{66}{Training deep networks}{equation.2.9.72}{}}
\newlabel{eq:dnnCostAbsoluteError}{{2.73}{66}{Training deep networks}{equation.2.9.73}{}}
\mph@setcol{ii:66}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.3}{The Backpropagation algorithm}}{67}{subsection.2.9.3}}
\newlabel{subsec:dnnBackpropagation}{{2.9.3}{67}{The Backpropagation algorithm}{subsection.2.9.3}{}}
\newlabel{eq:dnnBackpropActivation}{{2.74}{67}{The Backpropagation algorithm}{equation.2.9.74}{}}
\newlabel{eq:dnnBackprop1}{{2.75}{67}{The Backpropagation algorithm}{equation.2.9.75}{}}
\mph@setcol{ii:67}{\mph@nr}
\newlabel{eq:dnnBackprop2}{{2.76}{68}{The Backpropagation algorithm}{equation.2.9.76}{}}
\newlabel{eq:dnnBackprop3}{{2.77}{68}{The Backpropagation algorithm}{equation.2.9.77}{}}
\newlabel{eq:dnnBackprop4}{{2.78}{68}{The Backpropagation algorithm}{equation.2.9.78}{}}
\mph@setcol{ii:68}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.3.1}{What can go wrong with backpropagation ?}}{69}{subsubsection.2.9.3.1}}
\newlabel{subsubsec:dnnBackpropagationProblems}{{2.9.3.1}{69}{What can go wrong with backpropagation ?}{subsubsection.2.9.3.1}{}}
\mph@setcol{ii:69}{\mph@nr}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Left :\itshape  Backpropagation for a DNN with one neuron per layer. \normalfont  Right: \itshape  $L$-th layer is introduced by multiple activations in the previous layer if we have more than one neuron.\relax }}{70}{figure.caption.8}}
\newlabel{fig:backpropagation}{{2.5}{70}{Left :\itshape Backpropagation for a DNN with one neuron per layer. \normalfont Right: \itshape $L$-th layer is introduced by multiple activations in the previous layer if we have more than one neuron.\relax }{figure.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.4}{Regularizing neural networks and other practical considerations}}{70}{subsection.2.9.4}}
\newlabel{subsec:dnnRegularizingPractical}{{2.9.4}{70}{Regularizing neural networks and other practical considerations}{subsection.2.9.4}{}}
\mph@setcol{ii:70}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.4.1}{Implicit regularization using SGD: Initialization, hyper-parameter tuning, and Early Stopping}}{71}{subsubsection.2.9.4.1}}
\newlabel{subsubsec:dnnRegularizingPracticalSGDEarlyStopping}{{2.9.4.1}{71}{Implicit regularization using SGD: Initialization, hyper-parameter tuning, and Early Stopping}{subsubsection.2.9.4.1}{}}
\mph@setcol{ii:71}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.4.2}{Dropout}}{72}{subsubsection.2.9.4.2}}
\newlabel{subsubsec:dnnRegularizerPracticalDropout}{{2.9.4.2}{72}{Dropout}{subsubsection.2.9.4.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.4.3}{Batch Normalization}}{72}{subsubsection.2.9.4.3}}
\newlabel{subsubsec:dnnRegularizerPracticalBatchNorm}{{2.9.4.3}{72}{Batch Normalization}{subsubsection.2.9.4.3}{}}
\mph@setcol{ii:72}{\mph@nr}
\newlabel{eq:dnnRegularizerPracticalBatch1}{{2.79}{73}{Batch Normalization}{equation.2.9.79}{}}
\newlabel{eq:dnnRegularizerPracticalBatch2}{{2.80}{73}{Batch Normalization}{equation.2.9.80}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.5}{Deep neural networks in practice}}{73}{subsection.2.9.5}}
\newlabel{subsec:dnnPractice}{{2.9.5}{73}{Deep neural networks in practice}{subsection.2.9.5}{}}
\mph@setcol{ii:73}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.6}{Recipe DNNs}}{74}{subsection.2.9.6}}
\newlabel{subsec:dnnRecipe}{{2.9.6}{74}{Recipe DNNs}{subsection.2.9.6}{}}
\mph@setcol{ii:74}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.10}{Convolutional Neural Networks (CNNS)}}{75}{section.2.10}}
\newlabel{sec:cnn}{{2.10}{75}{Convolutional Neural Networks (CNNS)}{section.2.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.1}{Symmetries}}{75}{subsection.2.10.1}}
\newlabel{subsec:cnnSymmetries}{{2.10.1}{75}{Symmetries}{subsection.2.10.1}{}}
\mph@setcol{ii:75}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.2}{The structure of convolutional neural networks}}{76}{subsection.2.10.2}}
\newlabel{subsec:cnnStructure}{{2.10.2}{76}{The structure of convolutional neural networks}{subsection.2.10.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.2.1}{Architecture}}{76}{subsubsection.2.10.2.1}}
\mph@setcol{ii:76}{\mph@nr}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Architecture of a CNN: \itshape  The neurons in a CNN are arranged in three dimension: height $(H)$, width $(W)$, and depth $(D)$. For the input layer, the depth corresponds to the number of channels (in this case $3$ for RGB images). Neurons in the convolutional layers calculate the convolution of the image with a local spatial filter (e.g. $3\times 3$ pixel grid, times $3$ channels for first layer) at a given location in the spatial $(W,H)$-plane. The depth $D$ of the convolutional layer corresponds to the number of filters used in the convolutional layer. Neurons at the same depth correspond to the same filter. Neurons in the convolutional layer mix inputs at different depths but preserve the spatial location. Pooling layers perform a spatial coarse graining (pooling step) at each depth to give a smaller height and width while preserving the depth. The convolutional and pooling layers are followed by a fully connected layer and classifier (not shown).\relax }}{77}{figure.caption.9}}
\newlabel{fig:cnn}{{2.6}{77}{Architecture of a CNN: \itshape The neurons in a CNN are arranged in three dimension: height $(H)$, width $(W)$, and depth $(D)$. For the input layer, the depth corresponds to the number of channels (in this case $3$ for RGB images). Neurons in the convolutional layers calculate the convolution of the image with a local spatial filter (e.g. $3\times 3$ pixel grid, times $3$ channels for first layer) at a given location in the spatial $(W,H)$-plane. The depth $D$ of the convolutional layer corresponds to the number of filters used in the convolutional layer. Neurons at the same depth correspond to the same filter. Neurons in the convolutional layer mix inputs at different depths but preserve the spatial location. Pooling layers perform a spatial coarse graining (pooling step) at each depth to give a smaller height and width while preserving the depth. The convolutional and pooling layers are followed by a fully connected layer and classifier (not shown).\relax }{figure.caption.9}{}}
\mph@setcol{ii:77}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.2.2}{On convolutional layers}}{78}{subsubsection.2.10.2.2}}
\newlabel{subsubsec:cnnConvolutionalLayer}{{2.10.2.2}{78}{On convolutional layers}{subsubsection.2.10.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.2.3}{On pooling layers}}{78}{subsubsection.2.10.2.3}}
\newlabel{subsubsec:cnnPooling}{{2.10.2.3}{78}{On pooling layers}{subsubsection.2.10.2.3}{}}
\mph@setcol{ii:78}{\mph@nr}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Two examples to illustrate a one-dimensional convolutional layer with ReLU nonlinearity:\itshape  Convolutional layer for a spatial filter of size $F$ for a one-dimensional input of width $W$ with stride $S$ and padding $P$ followed by a ReLU non-linearity.\relax }}{79}{figure.caption.10}}
\newlabel{fig:cnnExample}{{2.7}{79}{Two examples to illustrate a one-dimensional convolutional layer with ReLU nonlinearity:\itshape Convolutional layer for a spatial filter of size $F$ for a one-dimensional input of width $W$ with stride $S$ and padding $P$ followed by a ReLU non-linearity.\relax }{figure.caption.10}{}}
\mph@setcol{ii:79}{\mph@nr}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Illustration of Max Pooling: \itshape  Illustration of max pooling over a $2\times 2$ region. Notice that pooling is done at each depth (vertical axis) separately. The number of outputs is halved along each dimension due to this coarse-graining.\relax }}{80}{figure.caption.11}}
\newlabel{fig:cnnPooling}{{2.8}{80}{Illustration of Max Pooling: \itshape Illustration of max pooling over a $2\times 2$ region. Notice that pooling is done at each depth (vertical axis) separately. The number of outputs is halved along each dimension due to this coarse-graining.\relax }{figure.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.2.4}{Classifying layer}}{80}{subsubsection.2.10.2.4}}
\mph@setcol{ii:80}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.3}{Pre-trained CNNs and transfer learning}}{81}{subsection.2.10.3}}
\newlabel{subsec:cnnTransferLearning}{{2.10.3}{81}{Pre-trained CNNs and transfer learning}{subsection.2.10.3}{}}
\mph@setcol{ii:81}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.11}{High-Level Concepts in Deep Neural Networks}}{82}{section.2.11}}
\newlabel{sec:dnn2}{{2.11}{82}{High-Level Concepts in Deep Neural Networks}{section.2.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.1}{Organizing deep learning workflows using the bias-variance tradeoff}}{82}{subsection.2.11.1}}
\newlabel{subsec:dnn2Workflow}{{2.11.1}{82}{Organizing deep learning workflows using the bias-variance tradeoff}{subsection.2.11.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces \relax }}{82}{figure.caption.12}}
\newlabel{fig:dnnworkflow}{{2.9}{82}{\relax }{figure.caption.12}{}}
\mph@setcol{ii:82}{\mph@nr}
\mph@setcol{ii:83}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.2}{Why NN are so successful: three high-level perspectives on neural networks}}{84}{subsection.2.11.2}}
\newlabel{subsec:dnn2Successful}{{2.11.2}{84}{Why NN are so successful: three high-level perspectives on neural networks}{subsection.2.11.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.11.2.1}{Neural networks as representation learning}}{84}{subsubsection.2.11.2.1}}
\newlabel{subsubsec:dnn2SuccessfulRepresentation}{{2.11.2.1}{84}{Neural networks as representation learning}{subsubsection.2.11.2.1}{}}
\mph@setcol{ii:84}{\mph@nr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.11.2.2}{Neural networks can exploit large amounts of data}}{85}{subsubsection.2.11.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.11.2.3}{Neural networks scale up well computationally}}{85}{subsubsection.2.11.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.3}{Limitations of supervised learning with deep networks}}{85}{subsection.2.11.3}}
\newlabel{sec:dnn2Limitations}{{2.11.3}{85}{Limitations of supervised learning with deep networks}{subsection.2.11.3}{}}
\mph@setcol{ii:85}{\mph@nr}
\mph@setcol{ii:86}{\mph@nr}
\@setckpt{Chapters/Supervised}{
\setcounter{page}{87}
\setcounter{equation}{80}
\setcounter{enumi}{4}
\setcounter{enumii}{2}
\setcounter{enumiii}{2}
\setcounter{enumiv}{0}
\setcounter{footnote}{19}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{11}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{0}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{lstnumber}{1}
\setcounter{listings}{0}
\setcounter{loldepth}{1}
\setcounter{Item}{109}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{66}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcbrastercolumn}{0}
\setcounter{tcbrasterrow}{0}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{statements}{0}
\setcounter{@todonotes@numberoftodonotes}{3}
\setcounter{lstlisting}{0}
\setcounter{section@level}{0}
}
