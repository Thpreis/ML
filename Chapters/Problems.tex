\chapter{Topic to work on}
\section{Temporal/Sequential Data}
What are techniques for dealing with temporal or sequential data. A powerful class of models for sequential data called Hidden Markov Models that utilize dynamical programming techniques have natural statistical physics interpretations in terms of transfer matrices. Recently, Recurrent NNs (RNNs) have become an important and powerful tool for dealing with sequence data. RNNs generalize many of the ideas discussed in \ref{sec:dnn} to deal with temporal data.
\section{Reinforcement learning}
Many of the most exciting developments in the last five years have come from combining reinforcement learning with DNNs. RL traces its origins to behaviourist psychology, when it was conceived as a way to explain and study reward-based decision making. RL is a field of ML, in which an angent learns how to master performing a specific task through an interaction with its environment. Depending on the reward it receives, the agent chooses to take an action affecting the environment, which in turn determines the value of the next received reward, and so on. The long-term goal of the agent is to maximize the cumulative expected return, thus improving its performance in the longer run.

\section{Support Vector Machines (SVMs) and Kernel Methods}
SVMs and kernel methods are a powerful set of techniques that work well when the amount of training data is limited.
























\chapter{Philosophical discussion of intelligence and consciousness}
AI, by design, is an ambiguous term that mixes aspirations with reality. It also conflates the statistical ideas that form the basis of modern ML with the more commonplace notions about what humans and behavioural scientists mean by intelligence. See \href{https://arxiv.org/abs/1604.00289}{Lake et al.} for an enlightening and important modern discussion of this distinction from a quantitative cognitive science point of view as well as \href{https://www.rand.org/pubs/papers/P3244.html}{Dreyfus} for a surprisingly relevant philosophy-based critique from $1965$.\\
\\
Almost all the techniques discussed here rely on optimizing a pre-specified objective function on a given dataset. Yet, we know that for large, complex models changing the data distribution or the goal can lead to an immediate degradation of performance. Deep networks have poor generalizations to even a sightly different context (the infamous Validation-Test set mismatch). This inability to abstract and generalize is a common criticism lobbied against branding modern ML techniques as AI (See Lake).\\
\\
The "Singularity" may not be coming but the advances in computing and the availability of large data sets likely ensure that the kind of statistical learning frameworks discussed are here to stay. Rather than a AGI, the kind of techniques presented here seem to be best suited for three important tasks:
\begin{enumerate}
	\item Automating prediction from lots of labelled examples in a narrowly-defined setting,
	\item learning how to parametrize and capture the correlations of complex probability distributions, and
	\item finding policies for tasks with well-defined goals and clear rules.
\end{enumerate}



\chapter{Social implication of ML}
Caution is in order when applying ML. Without foresight and accountability, the scale and scope of modern ML algorithms can lead to large scale unaccountable and undemocratic outcomes that can reinforce or even worsen existing inequality and inequities.\footnote{See book: Oâ€™Neil, Cathy (2017), Weapons of math destruction: How big
	data increases inequality and threatens democracy (Broad-
	way Books).}
When ML is used in a social context, abstract statistical relationships have real social consequences. False positives can mean the difference between life and death (e.g. signature drone strikes). ML algorithms, like al techniques, have important limitations and should be employed with great caution.\\
All algorithms involve inherent tradeoffs in fairness, \href{https://arxiv.org/abs/1609.05807}{see}. It is farm from clear how to make algorithms fair for all people involved. This is even more true with methods like DL that are hard to interpret. All ML algorithms have implicit assumptions and choices reflected in the datasets we use to the kind of functions we choose to optimize. It is important to remember that there is no "view from nowhere", compare \href{https://course.ccs.neu.edu/cs5150f14/readings/adam-artificialknowing.pdf}{1} and \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3078224}{2} - all ML algorithms reflect a point of view and a set of assumptions about the world we live in.








\chapter{Ideas for problems to work on}
\section{Physics problems}
\subsection{Cosmology}
\begin{enumerate}
	\item Maybe write a classification logistic regressor, or deep CNN to classify where dark matter halos have their boundary. It is very difficult to define a boundary of such a spread out object with confidence, maybe find criteria via classification ML technique ?
\item Maybe use density contrast concept from cosmology for density based clustering \ref{subsubsec:clusterPracticalDBSCAN}. Basically a mean field approach ?
\end{enumerate}
\subsection{QFT}
\begin{enumerate}
	\item Connected correlation functions describe n-point correlations between points (like covariance matrix). Can maybe find a motivation of describing connected correlations from qft, how do you infer connected correaltions from dataset ? Then you can find the set of one, two and higher n-point corr. functions for one class in your data, i.e. the ones which are correlated (inter cluster correlation possible ?Should be, compare \ref{fig:hierarchicalclustering}). Need to Wick rotate to Euclidean space for analogy. Can one use Feynman diagrams ?\\
	This should be interesting since you normally would consider relative ratios of probabilities for highly-complex unsupervised learning tasks as described in \ref{sec:varMFT}. The same is done in QFT, where you have cumulants which are normalized such that the partition function cancels out, i.e. we do not need to know the whole configuration.
	\item How can one infer distance and curvature from a picture. If i do a panorama picture, the points in the middle are further away (which is encoded in their curvature) and the ones on the side are rather far (at least the ones in the middle are squished), how can you generally tell curvature ? Can you construct a pixel triangle and infer local curvature from it or do you first need to find a metric in the local manifold describing the dataset locally ?
	\item Rather than understanding clusters as different densities  \ref{subsubsec:clusterPracticalDBSCAN} one could think of clusters like disconnectd (or maybe even connected if correlated ?) patches of a manifold. How can you fin the parametrization of this manifold, how its intrinsic metric and how the embedded metric ? Look at \href{https://arxiv.org/pdf/1802.03426.pdf}{manifold link}.
\end{enumerate}
\section{Ai safety}
\subsection{Questions}
\begin{enumerate}
	\item Can you define multiple hierarchical utility functions to implement Asimov's laws via reinforcement learning ?
\end{enumerate}
\section{Knobel-problems}
\subsection{Ideas}
\begin{enumerate}
	\item Write a CNN which looks at a Sudoku picture, converting it into a Sudoku data grid, then solve it via python ?
	
\end{enumerate}
\section{ML questions}
Is it possible to reconstruct the analytic representation of the function found by an agent to approximate the data we are interested in ?
What is the question here. First of all, normally we propose a model class to approximate the data in supervised learning problems. Using NN or unsupervised learning concepts, one however does not know the model class, one does not know the latent variables describing your problem. These agents readjust their weights and biases until the data is accurately represented or approximated. Could one translate this idea of weights and biases into an analytic function or model of functions ?\\
Could one use this to model the behaviour of a physical system, approximate the function and then let it be analytically represented to back-engineer the physical model describing the system ?