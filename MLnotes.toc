\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {british}{}
\defcounter {refsection}{0}\relax 
\vspace {-\cftbeforepartskip }
\babel@toc {british}{}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}{Introduction}}{3}{chapter.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.1}{Intro to AI}}{3}{section.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.1.1}{On governance/policy}}{4}{subsection.1.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.1.2}{ML}}{4}{subsection.1.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.1.2.1}{What is ML -- the schools of thought}}{4}{subsubsection.1.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.1.2.2}{What means training ?}}{6}{subsubsection.1.1.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.1.2.3}{Intriccacies with}}{7}{subsubsection.1.1.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.1.3}{Big Data}}{7}{subsection.1.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.2}{Math basics}}{7}{section.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.2.1}{Different possible types of learning}}{7}{subsection.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.1}{Supervised learning}}{7}{subsubsection.1.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.2}{Unsupervised learning}}{8}{subsubsection.1.2.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.3}{Reinforcement learning}}{9}{subsubsection.1.2.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.4}{On the learning process}}{9}{subsubsection.1.2.1.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.2.2}{Cost function}}{11}{subsection.1.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.2.1}{An example: The gradient descent algorithm}}{12}{subsubsection.1.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.2.3}{Data processing}}{13}{subsection.1.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}{Supervised and Unsupervised learning}}{15}{chapter.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.1}{More formal introduction into the idea of Machine Learning}}{15}{section.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.1}{Problem set-up and recipe}}{15}{subsection.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.1.1}{Ingredients}}{15}{subsubsection.2.1.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.1.2}{Recipe}}{16}{subsubsection.2.1.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.2}{Performance evaluation}}{16}{subsection.2.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.1}{Ingredients for performance evaluation}}{16}{subsubsection.2.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.2}{Another mathematical measure for performance evaluation}}{18}{subsubsection.2.1.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.3}{How to effectively do the performance evaluation}}{18}{subsubsection.2.1.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.4}{Where do the insights about how to do best practices come from ?}}{20}{subsubsection.2.1.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.2}{Statistics}}{21}{section.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.1}{Difference between estimation and prediction}}{21}{subsection.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.1}{Contrasting the two}}{21}{subsubsection.2.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.2}{Mathematical motivation to presented key ideas}}{22}{subsection.2.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.1}{Bias-Variance Decomposition for one Classifier}}{22}{subsubsection.2.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.2}{Bias-Variance Decomposition for Ensembles}}{24}{subsubsection.2.2.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.3}{Mathematical set-up of every problem ever}}{26}{subsection.2.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.1}{Set-up of the notation}}{26}{subsubsection.2.2.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.2}{Set-up of the problem}}{27}{subsubsection.2.2.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.3}{On statistical language}}{27}{subsubsection.2.2.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.4}{Language of optimization problems}}{28}{subsection.2.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.4.1}{Convexity}}{28}{subsubsection.2.2.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.3}{Overview of Bayesian Inference}}{28}{section.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.1}{Language}}{28}{subsection.2.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.1}{Connecting statistics and bayesian framework}}{30}{subsubsection.2.3.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.2}{Calculations}}{30}{subsection.2.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.3}{Estimation}}{31}{subsection.2.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.1}{Maximum Likelihood estimation}}{31}{subsubsection.2.3.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.2}{Maximum-a-Posteriori estimation}}{32}{subsubsection.2.3.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.3}{Out-of-Bag estimators}}{32}{subsubsection.2.3.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.4}{Bayesian view on regularization}}{32}{subsection.2.3.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.4.1}{MAP estimator in the context of regularization}}{33}{subsubsection.2.3.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.4}{Mathematical tools}}{33}{section.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.1}{Sampling methods}}{33}{subsection.2.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.1}{Empirical Bootstrapping}}{33}{subsubsection.2.4.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.2}{Algorithms}}{35}{subsection.2.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.2.1}{Decision Trees}}{35}{subsubsection.2.4.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.5}{Gradient descent}}{36}{section.2.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.1}{Simple gradient descent}}{36}{subsection.2.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.2}{Modified gradient descent}}{37}{subsection.2.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.5.2.1}{Stochastic gradient descent (SGD) with mini-batches}}{37}{subsubsection.2.5.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.5.2.2}{Algorithm gradient descent with momentum}}{37}{subsubsection.2.5.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.5.2.3}{Methods that use the second moment of the gradient}}{37}{subsubsection.2.5.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.3}{Practical tips for using GD}}{39}{subsection.2.5.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.6}{Linear regression}}{39}{section.2.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.1}{Least-square regressions -frequentist}}{39}{subsection.2.6.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.2}{Regularized Least-square regressions-frequentist}}{41}{subsection.2.6.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.1}{Ridge-Regression}}{42}{subsubsection.2.6.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.2}{LASSO and Sparse Regression}}{43}{subsubsection.2.6.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.3}{A note on LASSO and Ridge}}{44}{subsubsection.2.6.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.4}{Another note being a general comment on regularization}}{44}{subsubsection.2.6.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.5}{A general perspective on regularizers}}{45}{subsubsection.2.6.2.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.3}{Bayesian formulation of linear regression}}{45}{subsection.2.6.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.3.1}{Regularization }}{45}{subsubsection.2.6.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.4}{Outlook from linear regression}}{46}{subsection.2.6.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.7}{Logistic Regression}}{46}{section.2.7}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.1}{Mathematical set-up}}{46}{subsection.2.7.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.1.1}{Binary classification}}{46}{subsubsection.2.7.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.1.2}{Multi-class classification}}{46}{subsubsection.2.7.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.2}{Classifiers}}{47}{subsection.2.7.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.2.1}{On threshold function functions}}{47}{subsubsection.2.7.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.3}{Perceptron Learning Algorithm (PLA)}}{47}{subsection.2.7.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.3.1}{The algorithm}}{48}{subsubsection.2.7.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.4}{Definition of logistic regression - Bayesian}}{49}{subsection.2.7.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.4.1}{On the 2d Ising example}}{50}{subsubsection.2.7.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.4.2}{SUSY }}{50}{subsubsection.2.7.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.5}{SoftMax regression}}{50}{subsection.2.7.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.8}{Ensemble Methods - On combining models}}{51}{section.2.8}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.1}{Introduction}}{51}{subsection.2.8.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.1.1}{Motivation}}{51}{subsubsection.2.8.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.1.2}{Benefits of Ensemble Methods before diving in}}{52}{subsubsection.2.8.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.2}{Aggregate predictor methods - Bagging and Boosting}}{53}{subsection.2.8.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.2.1}{Bagging}}{53}{subsubsection.2.8.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.2.2}{Boosting}}{55}{subsubsection.2.8.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.3}{Random Forests}}{56}{subsection.2.8.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.4}{Gradient Boosted Trees and XGBoost}}{57}{subsection.2.8.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.4.1}{XGBoost}}{57}{subsubsection.2.8.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.9}{An Introduction to Feed-Forward Deep Neural Networks (DNNS)}}{60}{section.2.9}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.1}{Neural Network Basics}}{60}{subsection.2.9.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.1.1}{The basic building block: neurons}}{60}{subsubsection.2.9.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.1.2}{Layering neurons to build deep networks: network architecture}}{62}{subsubsection.2.9.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.1.3}{Further on network architecture: How many layers is adequate for a problem ?}}{65}{subsubsection.2.9.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.2}{Training deep networks}}{66}{subsection.2.9.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.3}{The Backpropagation algorithm}}{67}{subsection.2.9.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.3.1}{What can go wrong with backpropagation ?}}{69}{subsubsection.2.9.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.4}{Regularizing neural networks and other practical considerations}}{70}{subsection.2.9.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.4.1}{Implicit regularization using SGD: Initialization, hyper-parameter tuning, and Early Stopping}}{71}{subsubsection.2.9.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.4.2}{Dropout}}{72}{subsubsection.2.9.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.4.3}{Batch Normalization}}{72}{subsubsection.2.9.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.5}{Deep neural networks in practice}}{73}{subsection.2.9.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.6}{Recipe DNNs}}{74}{subsection.2.9.6}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.10}{Convolutional Neural Networks (CNNS)}}{75}{section.2.10}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.1}{Symmetries}}{75}{subsection.2.10.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.2}{The structure of convolutional neural networks}}{76}{subsection.2.10.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.2.1}{Architecture}}{76}{subsubsection.2.10.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.2.2}{On convolutional layers}}{78}{subsubsection.2.10.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.2.3}{On pooling layers}}{78}{subsubsection.2.10.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.2.4}{Classifying layer}}{80}{subsubsection.2.10.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.3}{Pre-trained CNNs and transfer learning}}{81}{subsection.2.10.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.11}{High-Level Concepts in Deep Neural Networks}}{82}{section.2.11}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.11.1}{Organizing deep learning workflows using the bias-variance tradeoff}}{82}{subsection.2.11.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.11.2}{Why NN are so successful: three high-level perspectives on neural networks}}{84}{subsection.2.11.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.1}{Neural networks as representation learning}}{84}{subsubsection.2.11.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.2}{Neural networks can exploit large amounts of data}}{85}{subsubsection.2.11.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.3}{Neural networks scale up well computationally}}{85}{subsubsection.2.11.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.11.3}{Limitations of supervised learning with deep networks}}{85}{subsection.2.11.3}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}{Unsupervised Learning}}{87}{chapter.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.1}{Dimensional Reduction and Data Visualization}}{87}{section.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.1}{Some of the challenges of high-dimensional data}}{87}{subsection.3.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.1}{High-dimensional data lives near the edge of sample space}}{87}{subsubsection.3.1.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.2}{Real-world data vs. uniform distribution}}{87}{subsubsection.3.1.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.3}{Intrinsic dimensionality and the crowding problem}}{88}{subsubsection.3.1.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.2}{Principal component analysis (PCA)}}{88}{subsection.3.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.2.1}{The theory}}{89}{subsubsection.3.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.3}{Multidimensional scaling}}{89}{subsection.3.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.1}{Metric MDS}}{90}{subsubsection.3.1.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.2}{Non-metric MDS}}{90}{subsubsection.3.1.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.3}{General comment}}{90}{subsubsection.3.1.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.4}{t-SNE}}{90}{subsection.3.1.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.4.1}{The theory}}{91}{subsubsection.3.1.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.4.2}{Important properties of $t$-SNE}}{92}{subsubsection.3.1.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.2}{Clustering}}{93}{section.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.1}{Practical clustering methods}}{93}{subsection.3.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.1.1}{$K$-means}}{94}{subsubsection.3.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.1.2}{Hierarchical clustering: Agglomerative methods}}{95}{subsubsection.3.2.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.1.3}{Density-based (DB) clustering}}{97}{subsubsection.3.2.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.2}{Clustering and Latent Variables via the Gaussian Mixture Models}}{98}{subsection.3.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.2.1}{Important concepts in unsupervised learning}}{98}{subsubsection.3.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.2.2}{Concepts visualized via the Gaussian Mixture Model (GMM)}}{99}{subsubsection.3.2.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.2.3}{Expectation Maximization (EM)}}{101}{subsubsection.3.2.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.3}{Clustering in high dimensions}}{102}{subsection.3.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.3.1}{Cluster validation in high dimensions}}{102}{subsubsection.3.2.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.3}{Variational Methods and Mean-Field Theory (MFT)}}{103}{section.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}{Ideas for problems to work on}}{105}{chapter.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}{Physics problems}}{105}{section.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.1.1}{Cosmology}}{105}{subsection.4.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.1.2}{QFT}}{105}{subsection.4.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}{Ai safety}}{105}{section.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.1}{Questions}}{105}{subsection.4.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.3}{Knobel-problems}}{105}{section.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.1}{Ideas}}{105}{subsection.4.3.1}
