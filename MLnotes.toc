\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {british}{}
\defcounter {refsection}{0}\relax 
\vspace {-\cftbeforepartskip }
\babel@toc {british}{}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}{Introduction}}{3}{chapter.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.1}{Intro to AI}}{3}{section.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.1.1}{On governance/policy}}{4}{subsection.1.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.1.2}{ML}}{4}{subsection.1.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.1.2.1}{What is ML -- the schools of thought}}{4}{subsubsection.1.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.1.2.2}{What means training ?}}{6}{subsubsection.1.1.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.1.2.3}{Intriccacies with}}{7}{subsubsection.1.1.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.1.3}{Big Data}}{7}{subsection.1.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.2}{Math basics}}{7}{section.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.2.1}{Different possible types of learning}}{7}{subsection.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.1}{Supervised learning}}{7}{subsubsection.1.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.2}{Unsupervised learning}}{8}{subsubsection.1.2.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.3}{Reinforcement learning}}{9}{subsubsection.1.2.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.1.4}{On the learning process}}{9}{subsubsection.1.2.1.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.2.2}{Cost function}}{11}{subsection.1.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {1.2.2.1}{An example: The gradient descent algorithm}}{12}{subsubsection.1.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.2.3}{Data processing}}{13}{subsection.1.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}{Supervised learning}}{15}{chapter.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.1}{More formal introduction into the idea of Machine Learning}}{15}{section.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.1}{Problem set-up and recipe}}{15}{subsection.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.1.1}{Ingredients}}{15}{subsubsection.2.1.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.1.2}{Recipe}}{16}{subsubsection.2.1.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.2}{Performance evaluation}}{16}{subsection.2.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.1}{Ingredients for performance evaluation}}{16}{subsubsection.2.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.2}{Another mathematical measure for performance evaluation}}{18}{subsubsection.2.1.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.3}{How to effectively do the performance evaluation}}{18}{subsubsection.2.1.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.2.4}{Where do the insights about how to do best practices come from ?}}{20}{subsubsection.2.1.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.2}{Statistics}}{21}{section.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.1}{Difference between estimation and prediction}}{21}{subsection.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.1}{Contrasting the two}}{21}{subsubsection.2.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.2}{Mathematical motivation to presented key ideas}}{22}{subsection.2.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.1}{Bias-Variance Decomposition for one Classifier}}{22}{subsubsection.2.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.2}{Bias-Variance Decomposition for Ensembles}}{24}{subsubsection.2.2.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.3}{Mathematical set-up of every problem ever}}{26}{subsection.2.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.1}{Set-up of the notation}}{26}{subsubsection.2.2.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.2}{Set-up of the problem}}{27}{subsubsection.2.2.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.3}{On statistical language}}{27}{subsubsection.2.2.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.4}{Language of optimization problems}}{28}{subsection.2.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.4.1}{Convexity}}{28}{subsubsection.2.2.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.3}{Overview of Bayesian Inference}}{28}{section.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.1}{Language}}{28}{subsection.2.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.1}{Connecting statistics and bayesian framework}}{30}{subsubsection.2.3.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.2}{Calculations}}{30}{subsection.2.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.3}{Estimation}}{31}{subsection.2.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.1}{Maximum Likelihood estimation}}{31}{subsubsection.2.3.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.2}{Maximum-a-Posteriori estimation}}{32}{subsubsection.2.3.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.3}{Out-of-Bag estimators}}{32}{subsubsection.2.3.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.4}{Bayesian view on regularization}}{32}{subsection.2.3.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.4.1}{MAP estimator in the context of regularization}}{33}{subsubsection.2.3.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.4}{Information theory}}{33}{section.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.1}{Entropy as a measure of information}}{33}{subsection.2.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.1}{Probabilities introduction}}{33}{subsubsection.2.4.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.2}{Shannon's information entropy}}{34}{subsubsection.2.4.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.2}{Conditional entropy}}{36}{subsection.2.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.3}{Joint entropy}}{36}{subsection.2.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.4}{Mutual information}}{36}{subsection.2.4.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.4.1}{Relative Entropy or Kullback-Leibler divergence}}{37}{subsubsection.2.4.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.4.2}{Continuum limit}}{38}{subsubsection.2.4.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.5}{Fisher information metric}}{38}{subsection.2.4.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.5}{Mathematical tools}}{39}{section.2.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.1}{Sampling methods}}{39}{subsection.2.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.5.1.1}{Empirical Bootstrapping}}{39}{subsubsection.2.5.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.2}{Algorithms}}{40}{subsection.2.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.5.2.1}{Decision Trees}}{40}{subsubsection.2.5.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.6}{Gradient descent}}{42}{section.2.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.1}{Simple gradient descent}}{42}{subsection.2.6.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.2}{Modified gradient descent}}{43}{subsection.2.6.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.1}{Stochastic gradient descent (SGD) with mini-batches}}{43}{subsubsection.2.6.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.2}{Algorithm gradient descent with momentum}}{43}{subsubsection.2.6.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.2.3}{Methods that use the second moment of the gradient}}{43}{subsubsection.2.6.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.3}{Practical tips for using GD}}{45}{subsection.2.6.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.7}{Linear regression}}{45}{section.2.7}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.1}{Least-square regressions -frequentist}}{45}{subsection.2.7.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.2}{Regularized Least-square regressions-frequentist}}{47}{subsection.2.7.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.2.1}{Ridge-Regression}}{48}{subsubsection.2.7.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.2.2}{LASSO and Sparse Regression}}{49}{subsubsection.2.7.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.2.3}{A note on LASSO and Ridge}}{50}{subsubsection.2.7.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.2.4}{Another note being a general comment on regularization}}{50}{subsubsection.2.7.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.2.5}{A general perspective on regularizers}}{51}{subsubsection.2.7.2.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.3}{Bayesian formulation of linear regression}}{51}{subsection.2.7.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.3.1}{Regularization }}{51}{subsubsection.2.7.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.4}{Outlook from linear regression}}{52}{subsection.2.7.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.8}{Logistic Regression}}{52}{section.2.8}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.1}{Mathematical set-up}}{52}{subsection.2.8.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.1.1}{Binary classification}}{52}{subsubsection.2.8.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.1.2}{Multi-class classification}}{52}{subsubsection.2.8.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.2}{Classifiers}}{53}{subsection.2.8.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.2.1}{On threshold function functions}}{53}{subsubsection.2.8.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.3}{Perceptron Learning Algorithm (PLA)}}{53}{subsection.2.8.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.3.1}{The algorithm}}{54}{subsubsection.2.8.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.4}{Definition of logistic regression - Bayesian}}{55}{subsection.2.8.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.4.1}{On the 2d Ising example}}{56}{subsubsection.2.8.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.8.4.2}{SUSY }}{56}{subsubsection.2.8.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.8.5}{SoftMax regression}}{56}{subsection.2.8.5}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.9}{Ensemble Methods - On combining models}}{57}{section.2.9}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.1}{Introduction}}{57}{subsection.2.9.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.1.1}{Motivation}}{57}{subsubsection.2.9.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.1.2}{Benefits of Ensemble Methods before diving in}}{58}{subsubsection.2.9.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.2}{Aggregate predictor methods - Bagging and Boosting}}{59}{subsection.2.9.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.2.1}{Bagging}}{59}{subsubsection.2.9.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.2.2}{Boosting}}{61}{subsubsection.2.9.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.3}{Random Forests}}{62}{subsection.2.9.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.9.4}{Gradient Boosted Trees and XGBoost}}{63}{subsection.2.9.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.9.4.1}{XGBoost}}{63}{subsubsection.2.9.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.10}{An Introduction to Feed-Forward Deep Neural Networks (DNNS)}}{66}{section.2.10}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.1}{Neural Network Basics}}{66}{subsection.2.10.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.1.1}{The basic building block: neurons}}{66}{subsubsection.2.10.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.1.2}{Layering neurons to build deep networks: network architecture}}{68}{subsubsection.2.10.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.1.3}{Further on network architecture: How many layers is adequate for a problem ?}}{71}{subsubsection.2.10.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.2}{Training deep networks}}{72}{subsection.2.10.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.3}{The Backpropagation algorithm}}{73}{subsection.2.10.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.3.1}{What can go wrong with backpropagation ?}}{75}{subsubsection.2.10.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.4}{Regularizing neural networks and other practical considerations}}{76}{subsection.2.10.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.4.1}{Implicit regularization using SGD: Initialization, hyper-parameter tuning, and Early Stopping}}{77}{subsubsection.2.10.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.4.2}{Dropout}}{78}{subsubsection.2.10.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.10.4.3}{Batch Normalization}}{78}{subsubsection.2.10.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.5}{Deep neural networks in practice}}{79}{subsection.2.10.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.10.6}{Recipe DNNs}}{80}{subsection.2.10.6}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.11}{Convolutional Neural Networks (CNNS)}}{81}{section.2.11}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.11.1}{Symmetries}}{81}{subsection.2.11.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.11.2}{The structure of convolutional neural networks}}{82}{subsection.2.11.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.1}{Architecture}}{82}{subsubsection.2.11.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.2}{On convolutional layers}}{84}{subsubsection.2.11.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.3}{On pooling layers}}{84}{subsubsection.2.11.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.11.2.4}{Classifying layer}}{86}{subsubsection.2.11.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.11.3}{Pre-trained CNNs and transfer learning}}{87}{subsection.2.11.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.12}{High-Level Concepts in Deep Neural Networks}}{88}{section.2.12}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.12.1}{Organizing deep learning workflows using the bias-variance tradeoff}}{88}{subsection.2.12.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.12.2}{Why NN are so successful: three high-level perspectives on neural networks}}{90}{subsection.2.12.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.12.2.1}{Neural networks as representation learning}}{90}{subsubsection.2.12.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.12.2.2}{Neural networks can exploit large amounts of data}}{91}{subsubsection.2.12.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.12.2.3}{Neural networks scale up well computationally}}{91}{subsubsection.2.12.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.12.3}{Limitations of supervised learning with deep networks}}{91}{subsection.2.12.3}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}{Unsupervised Learning}}{93}{chapter.3}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.1}{Dimensional Reduction and Data Visualization}}{93}{section.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.1}{Some of the challenges of high-dimensional data}}{93}{subsection.3.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.1}{High-dimensional data lives near the edge of sample space}}{93}{subsubsection.3.1.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.2}{Real-world data vs. uniform distribution}}{93}{subsubsection.3.1.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.3}{Intrinsic dimensionality and the crowding problem}}{94}{subsubsection.3.1.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.2}{Principal component analysis (PCA)}}{95}{subsection.3.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.2.1}{The theory}}{95}{subsubsection.3.1.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.3}{Multidimensional scaling}}{96}{subsection.3.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.1}{Metric MDS}}{96}{subsubsection.3.1.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.2}{Non-metric MDS}}{96}{subsubsection.3.1.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.3}{General comment}}{96}{subsubsection.3.1.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.4}{t-SNE}}{97}{subsection.3.1.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.4.1}{The theory}}{97}{subsubsection.3.1.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.4.2}{Important properties of $t$-SNE}}{98}{subsubsection.3.1.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.2}{Clustering}}{99}{section.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.1}{Practical clustering methods}}{100}{subsection.3.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.1.1}{$K$-means}}{100}{subsubsection.3.2.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.1.2}{Hierarchical clustering: Agglomerative methods}}{101}{subsubsection.3.2.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.1.3}{Density-based (DB) clustering}}{103}{subsubsection.3.2.1.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.2}{Clustering and Latent Variables via the Gaussian Mixture Models}}{104}{subsection.3.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.2.1}{Important concepts in unsupervised learning}}{104}{subsubsection.3.2.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.2.2}{Concepts visualized via the Gaussian Mixture Model (GMM)}}{106}{subsubsection.3.2.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.3}{Clustering in high dimensions}}{107}{subsection.3.2.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.3.1}{Cluster validation in high dimensions}}{108}{subsubsection.3.2.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.4}{Practical considerations of clustering methods - when do we use which method ?}}{108}{subsection.3.2.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.3}{Variational Methods and Mean-Field Theory (MFT)}}{108}{section.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.3.1}{Variational methods introduction}}{108}{subsection.3.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.3.2}{Variational mean-field theory via Ising model}}{109}{subsection.3.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.3.2.1}{Important concepts of MFT}}{109}{subsubsection.3.3.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.3.2.2}{On the example of the Ising model}}{110}{subsubsection.3.3.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.3.3}{Expectation Maximization (EM)}}{111}{subsection.3.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.3.3.1}{EM in a nutshell - mathematical}}{111}{subsubsection.3.3.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.3.3.2}{EM via statistical physics}}{112}{subsubsection.3.3.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.4}{Energy Based Models: Maximum Entropy (MaxEnt) Principle, Generative Models, and Boltmann Learning}}{114}{section.3.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.1}{Introduction - Why do we need another class of models}}{114}{subsection.3.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.2}{An overview of energy-based generative models}}{114}{subsection.3.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.2.1}{The idea}}{114}{subsubsection.3.4.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.2.2}{Generative models in practice}}{115}{subsubsection.3.4.2.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.3}{Maximum entropy models: the simplest energy-based generative models}}{116}{subsection.3.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.3.1}{Derivation of the theory}}{116}{subsubsection.3.4.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.3.2}{From statistical mechanics to ML}}{118}{subsubsection.3.4.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.3.3}{Generalized Ising models from MaxEnt}}{119}{subsubsection.3.4.3.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.4}{Cost functions for training energy-based models}}{120}{subsection.3.4.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.4.1}{Maximum likelihood}}{121}{subsubsection.3.4.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.4.2}{Regularization}}{121}{subsubsection.3.4.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.5}{Computing gradients}}{122}{subsection.3.4.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.5.1}{How does one actually compute the gradient then ?}}{123}{subsubsection.3.4.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.6}{Summary of the training procedure}}{124}{subsection.3.4.6}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.6.1}{Practical tips}}{124}{subsubsection.3.4.6.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.5}{Deep Generative Models: Hidden Variables and Restricted Boltzmann Machines (RBMs)}}{125}{section.3.5}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.5.1}{Why hidden (latent) variables ?}}{125}{subsection.3.5.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.1.1}{Idea}}{125}{subsubsection.3.5.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.1.2}{For example - the Ising model}}{126}{subsubsection.3.5.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.5.2}{Restricted Boltzmann Machines (RBMs)}}{127}{subsection.3.5.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.2.1}{Content of an RBM}}{128}{subsubsection.3.5.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.5.3}{Training RBMs}}{129}{subsection.3.5.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.3.1}{Gibbs sampling and contrastive divergence (CD)}}{129}{subsubsection.3.5.3.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.3.2}{Practical Considerations}}{130}{subsubsection.3.5.3.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.5.4}{Deep Boltzmann Machine}}{131}{subsection.3.5.4}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.4.1}{The idea}}{131}{subsubsection.3.5.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.4.2}{Training a DBM}}{132}{subsubsection.3.5.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}{Ideas for problems to work on}}{135}{chapter.4}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}{Physics problems}}{135}{section.4.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.1.1}{Cosmology}}{135}{subsection.4.1.1}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.1.2}{QFT}}{135}{subsection.4.1.2}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}{Ai safety}}{136}{section.4.2}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.1}{Questions}}{136}{subsection.4.2.1}
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.3}{Knobel-problems}}{136}{section.4.3}
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.1}{Ideas}}{136}{subsection.4.3.1}
