\BOOKMARK [1][-]{tableofcontents.1}{Contents}{}% 1
\BOOKMARK [1][-]{lof.1}{List\040of\040Figures}{}% 2
\BOOKMARK [1][-]{lot.1}{List\040of\040Tables}{}% 3
\BOOKMARK [1][-]{lol.1}{Listings}{}% 4
\BOOKMARK [1][-]{acronyms.1}{Acronyms}{}% 5
\BOOKMARK [0][]{chapter.1}{1\040Introduction}{}% 6
\BOOKMARK [1][-]{section.1.1}{1.1\040Intro\040to\040AI}{chapter.1}% 7
\BOOKMARK [2][-]{subsection.1.1.1}{1.1.1\040On\040governance/policy}{section.1.1}% 8
\BOOKMARK [2][-]{subsection.1.1.2}{1.1.2\040ML}{section.1.1}% 9
\BOOKMARK [2][-]{subsection.1.1.3}{1.1.3\040Big\040Data}{section.1.1}% 10
\BOOKMARK [1][-]{section.1.2}{1.2\040Math\040basics}{chapter.1}% 11
\BOOKMARK [2][-]{subsection.1.2.1}{1.2.1\040Different\040possible\040types\040of\040learning}{section.1.2}% 12
\BOOKMARK [2][-]{subsection.1.2.2}{1.2.2\040Cost\040function}{section.1.2}% 13
\BOOKMARK [2][-]{subsection.1.2.3}{1.2.3\040Data\040processing}{section.1.2}% 14
\BOOKMARK [0][]{chapter.2}{2\040Supervised\040learning}{}% 15
\BOOKMARK [1][-]{section.2.1}{2.1\040More\040formal\040introduction\040into\040the\040idea\040of\040Machine\040Learning}{chapter.2}% 16
\BOOKMARK [2][-]{subsection.2.1.1}{2.1.1\040Problem\040set-up\040and\040recipe}{section.2.1}% 17
\BOOKMARK [2][-]{subsection.2.1.2}{2.1.2\040Performance\040evaluation}{section.2.1}% 18
\BOOKMARK [1][-]{section.2.2}{2.2\040Statistics}{chapter.2}% 19
\BOOKMARK [2][-]{subsection.2.2.1}{2.2.1\040Difference\040between\040estimation\040and\040prediction}{section.2.2}% 20
\BOOKMARK [2][-]{subsection.2.2.2}{2.2.2\040Mathematical\040motivation\040to\040presented\040key\040ideas}{section.2.2}% 21
\BOOKMARK [2][-]{subsection.2.2.3}{2.2.3\040Mathematical\040set-up\040of\040every\040problem\040ever}{section.2.2}% 22
\BOOKMARK [2][-]{subsection.2.2.4}{2.2.4\040Language\040of\040optimization\040problems}{section.2.2}% 23
\BOOKMARK [1][-]{section.2.3}{2.3\040Overview\040of\040Bayesian\040Inference}{chapter.2}% 24
\BOOKMARK [2][-]{subsection.2.3.1}{2.3.1\040Language}{section.2.3}% 25
\BOOKMARK [2][-]{subsection.2.3.2}{2.3.2\040Calculations}{section.2.3}% 26
\BOOKMARK [2][-]{subsection.2.3.3}{2.3.3\040Estimation}{section.2.3}% 27
\BOOKMARK [2][-]{subsection.2.3.4}{2.3.4\040Bayesian\040view\040on\040regularization}{section.2.3}% 28
\BOOKMARK [1][-]{section.2.4}{2.4\040Information\040theory}{chapter.2}% 29
\BOOKMARK [2][-]{subsection.2.4.1}{2.4.1\040Entropy\040as\040a\040measure\040of\040information}{section.2.4}% 30
\BOOKMARK [2][-]{subsection.2.4.2}{2.4.2\040Conditional\040entropy}{section.2.4}% 31
\BOOKMARK [2][-]{subsection.2.4.3}{2.4.3\040Joint\040entropy}{section.2.4}% 32
\BOOKMARK [2][-]{subsection.2.4.4}{2.4.4\040Mutual\040information}{section.2.4}% 33
\BOOKMARK [2][-]{subsection.2.4.5}{2.4.5\040Fisher\040information\040metric}{section.2.4}% 34
\BOOKMARK [1][-]{section.2.5}{2.5\040Mathematical\040tools}{chapter.2}% 35
\BOOKMARK [2][-]{subsection.2.5.1}{2.5.1\040Sampling\040methods}{section.2.5}% 36
\BOOKMARK [2][-]{subsection.2.5.2}{2.5.2\040Algorithms}{section.2.5}% 37
\BOOKMARK [1][-]{section.2.6}{2.6\040Gradient\040descent}{chapter.2}% 38
\BOOKMARK [2][-]{subsection.2.6.1}{2.6.1\040Simple\040gradient\040descent}{section.2.6}% 39
\BOOKMARK [2][-]{subsection.2.6.2}{2.6.2\040Modified\040gradient\040descent}{section.2.6}% 40
\BOOKMARK [2][-]{subsection.2.6.3}{2.6.3\040Practical\040tips\040for\040using\040GD}{section.2.6}% 41
\BOOKMARK [1][-]{section.2.7}{2.7\040Linear\040regression}{chapter.2}% 42
\BOOKMARK [2][-]{subsection.2.7.1}{2.7.1\040Least-square\040regressions\040-frequentist}{section.2.7}% 43
\BOOKMARK [2][-]{subsection.2.7.2}{2.7.2\040Regularized\040Least-square\040regressions-frequentist}{section.2.7}% 44
\BOOKMARK [2][-]{subsection.2.7.3}{2.7.3\040Bayesian\040formulation\040of\040linear\040regression}{section.2.7}% 45
\BOOKMARK [2][-]{subsection.2.7.4}{2.7.4\040Outlook\040from\040linear\040regression}{section.2.7}% 46
\BOOKMARK [1][-]{section.2.8}{2.8\040Logistic\040Regression}{chapter.2}% 47
\BOOKMARK [2][-]{subsection.2.8.1}{2.8.1\040Mathematical\040set-up}{section.2.8}% 48
\BOOKMARK [2][-]{subsection.2.8.2}{2.8.2\040Classifiers}{section.2.8}% 49
\BOOKMARK [2][-]{subsection.2.8.3}{2.8.3\040Perceptron\040Learning\040Algorithm\040\(PLA\)}{section.2.8}% 50
\BOOKMARK [2][-]{subsection.2.8.4}{2.8.4\040Definition\040of\040logistic\040regression\040-\040Bayesian}{section.2.8}% 51
\BOOKMARK [2][-]{subsection.2.8.5}{2.8.5\040SoftMax\040regression}{section.2.8}% 52
\BOOKMARK [1][-]{section.2.9}{2.9\040Ensemble\040Methods\040-\040On\040combining\040models}{chapter.2}% 53
\BOOKMARK [2][-]{subsection.2.9.1}{2.9.1\040Introduction}{section.2.9}% 54
\BOOKMARK [2][-]{subsection.2.9.2}{2.9.2\040Aggregate\040predictor\040methods\040-\040Bagging\040and\040Boosting}{section.2.9}% 55
\BOOKMARK [2][-]{subsection.2.9.3}{2.9.3\040Random\040Forests}{section.2.9}% 56
\BOOKMARK [2][-]{subsection.2.9.4}{2.9.4\040Gradient\040Boosted\040Trees\040and\040XGBoost}{section.2.9}% 57
\BOOKMARK [1][-]{section.2.10}{2.10\040An\040Introduction\040to\040Feed-Forward\040Deep\040Neural\040Networks\040\(DNNS\)}{chapter.2}% 58
\BOOKMARK [2][-]{subsection.2.10.1}{2.10.1\040Neural\040Network\040Basics}{section.2.10}% 59
\BOOKMARK [2][-]{subsection.2.10.2}{2.10.2\040Training\040deep\040networks}{section.2.10}% 60
\BOOKMARK [2][-]{subsection.2.10.3}{2.10.3\040The\040Backpropagation\040algorithm}{section.2.10}% 61
\BOOKMARK [2][-]{subsection.2.10.4}{2.10.4\040Regularizing\040neural\040networks\040and\040other\040practical\040considerations}{section.2.10}% 62
\BOOKMARK [2][-]{subsection.2.10.5}{2.10.5\040Deep\040neural\040networks\040in\040practice}{section.2.10}% 63
\BOOKMARK [2][-]{subsection.2.10.6}{2.10.6\040Recipe\040DNNs}{section.2.10}% 64
\BOOKMARK [1][-]{section.2.11}{2.11\040Convolutional\040Neural\040Networks\040\(CNNS\)}{chapter.2}% 65
\BOOKMARK [2][-]{subsection.2.11.1}{2.11.1\040Symmetries}{section.2.11}% 66
\BOOKMARK [2][-]{subsection.2.11.2}{2.11.2\040The\040structure\040of\040convolutional\040neural\040networks}{section.2.11}% 67
\BOOKMARK [2][-]{subsection.2.11.3}{2.11.3\040Pre-trained\040CNNs\040and\040transfer\040learning}{section.2.11}% 68
\BOOKMARK [1][-]{section.2.12}{2.12\040High-Level\040Concepts\040in\040Deep\040Neural\040Networks}{chapter.2}% 69
\BOOKMARK [2][-]{subsection.2.12.1}{2.12.1\040Organizing\040deep\040learning\040workflows\040using\040the\040bias-variance\040tradeoff}{section.2.12}% 70
\BOOKMARK [2][-]{subsection.2.12.2}{2.12.2\040Why\040NN\040are\040so\040successful:\040three\040high-level\040perspectives\040on\040neural\040networks}{section.2.12}% 71
\BOOKMARK [2][-]{subsection.2.12.3}{2.12.3\040Limitations\040of\040supervised\040learning\040with\040deep\040networks}{section.2.12}% 72
\BOOKMARK [0][]{chapter.3}{3\040Unsupervised\040Learning}{}% 73
\BOOKMARK [1][-]{section.3.1}{3.1\040Dimensional\040Reduction\040and\040Data\040Visualization}{chapter.3}% 74
\BOOKMARK [2][-]{subsection.3.1.1}{3.1.1\040Some\040of\040the\040challenges\040of\040high-dimensional\040data}{section.3.1}% 75
\BOOKMARK [2][-]{subsection.3.1.2}{3.1.2\040Principal\040component\040analysis\040\(PCA\)}{section.3.1}% 76
\BOOKMARK [2][-]{subsection.3.1.3}{3.1.3\040Multidimensional\040scaling}{section.3.1}% 77
\BOOKMARK [2][-]{subsection.3.1.4}{3.1.4\040t-SNE}{section.3.1}% 78
\BOOKMARK [1][-]{section.3.2}{3.2\040Clustering}{chapter.3}% 79
\BOOKMARK [2][-]{subsection.3.2.1}{3.2.1\040Practical\040clustering\040methods}{section.3.2}% 80
\BOOKMARK [2][-]{subsection.3.2.2}{3.2.2\040Clustering\040and\040Latent\040Variables\040via\040the\040Gaussian\040Mixture\040Models}{section.3.2}% 81
\BOOKMARK [2][-]{subsection.3.2.3}{3.2.3\040Clustering\040in\040high\040dimensions}{section.3.2}% 82
\BOOKMARK [2][-]{subsection.3.2.4}{3.2.4\040Practical\040considerations\040of\040clustering\040methods\040-\040when\040do\040we\040use\040which\040method\040?}{section.3.2}% 83
\BOOKMARK [1][-]{section.3.3}{3.3\040Variational\040Methods\040and\040Mean-Field\040Theory\040\(MFT\)}{chapter.3}% 84
\BOOKMARK [2][-]{subsection.3.3.1}{3.3.1\040Variational\040methods\040introduction}{section.3.3}% 85
\BOOKMARK [2][-]{subsection.3.3.2}{3.3.2\040Variational\040mean-field\040theory\040via\040Ising\040model}{section.3.3}% 86
\BOOKMARK [2][-]{subsection.3.3.3}{3.3.3\040Expectation\040Maximization\040\(EM\)}{section.3.3}% 87
\BOOKMARK [1][-]{section.3.4}{3.4\040Energy\040Based\040Models:\040Maximum\040Entropy\040\(MaxEnt\)\040Principle,\040Generative\040Models,\040and\040Boltmann\040Learning}{chapter.3}% 88
\BOOKMARK [2][-]{subsection.3.4.1}{3.4.1\040Introduction\040-\040Why\040do\040we\040need\040another\040class\040of\040models}{section.3.4}% 89
\BOOKMARK [2][-]{subsection.3.4.2}{3.4.2\040An\040overview\040of\040energy-based\040generative\040models}{section.3.4}% 90
\BOOKMARK [2][-]{subsection.3.4.3}{3.4.3\040Maximum\040entropy\040models:\040the\040simplest\040energy-based\040generative\040models}{section.3.4}% 91
\BOOKMARK [2][-]{subsection.3.4.4}{3.4.4\040Cost\040functions\040for\040training\040energy-based\040models}{section.3.4}% 92
\BOOKMARK [2][-]{subsection.3.4.5}{3.4.5\040Computing\040gradients}{section.3.4}% 93
\BOOKMARK [2][-]{subsection.3.4.6}{3.4.6\040Summary\040of\040the\040training\040procedure}{section.3.4}% 94
\BOOKMARK [1][-]{section.3.5}{3.5\040Deep\040Generative\040Models:\040Hidden\040Variables\040and\040Restricted\040Boltzmann\040Machines\040\(RBMs\)}{chapter.3}% 95
\BOOKMARK [2][-]{subsection.3.5.1}{3.5.1\040Why\040hidden\040\(latent\)\040variables\040?}{section.3.5}% 96
\BOOKMARK [2][-]{subsection.3.5.2}{3.5.2\040Restricted\040Boltzmann\040Machines\040\(RBMs\)}{section.3.5}% 97
\BOOKMARK [2][-]{subsection.3.5.3}{3.5.3\040Training\040RBMs}{section.3.5}% 98
\BOOKMARK [2][-]{subsection.3.5.4}{3.5.4\040Deep\040Boltzmann\040Machine}{section.3.5}% 99
\BOOKMARK [1][-]{section.3.6}{3.6\040Variational\040AutoEncoders\040\(VAES\)\040and\040introduction\040to\040Generative\040Adversarial\040Networks\040\(GANS\)}{chapter.3}% 100
\BOOKMARK [2][-]{subsection.3.6.1}{3.6.1\040The\040limitations\040of\040maximizing\040Likelihood}{section.3.6}% 101
\BOOKMARK [2][-]{subsection.3.6.2}{3.6.2\040Generative\040models\040and\040adversarial\040learning}{section.3.6}% 102
\BOOKMARK [2][-]{subsection.3.6.3}{3.6.3\040Variational\040Autoencoders}{section.3.6}% 103
\BOOKMARK [2][-]{subsection.3.6.4}{3.6.4\040VAE\040with\040Gaussian\040latent\040variables\040and\040Gaussian\040encoder}{section.3.6}% 104
\BOOKMARK [1][-]{section.3.7}{3.7\040Generative\040Adversarial\040Networks\040in\040detail}{chapter.3}% 105
\BOOKMARK [2][-]{subsection.3.7.1}{3.7.1\040Introduction\040-\040recap\040of\040generative\040models\040using\040MLE}{section.3.7}% 106
\BOOKMARK [2][-]{subsection.3.7.2}{3.7.2\040How\040do\040GANs\040work\040?}{section.3.7}% 107
\BOOKMARK [2][-]{subsection.3.7.3}{3.7.3\040..}{section.3.7}% 108
\BOOKMARK [0][]{chapter.4}{4\040Topic\040to\040work\040on}{}% 109
\BOOKMARK [1][-]{section.4.1}{4.1\040Temporal/Sequential\040Data}{chapter.4}% 110
\BOOKMARK [1][-]{section.4.2}{4.2\040Reinforcement\040learning}{chapter.4}% 111
\BOOKMARK [1][-]{section.4.3}{4.3\040Support\040Vector\040Machines\040\(SVMs\)\040and\040Kernel\040Methods}{chapter.4}% 112
\BOOKMARK [0][]{chapter.5}{5\040Philosophical\040discussion\040of\040intelligence\040and\040consciousness}{}% 113
\BOOKMARK [0][]{chapter.6}{6\040Social\040implication\040of\040ML}{}% 114
\BOOKMARK [0][]{chapter.7}{7\040Ideas\040for\040problems\040to\040work\040on}{}% 115
\BOOKMARK [1][-]{section.7.1}{7.1\040Physics\040problems}{chapter.7}% 116
\BOOKMARK [2][-]{subsection.7.1.1}{7.1.1\040Cosmology}{section.7.1}% 117
\BOOKMARK [2][-]{subsection.7.1.2}{7.1.2\040QFT}{section.7.1}% 118
\BOOKMARK [1][-]{section.7.2}{7.2\040Ai\040safety}{chapter.7}% 119
\BOOKMARK [2][-]{subsection.7.2.1}{7.2.1\040Questions}{section.7.2}% 120
\BOOKMARK [1][-]{section.7.3}{7.3\040Knobel-problems}{chapter.7}% 121
\BOOKMARK [2][-]{subsection.7.3.1}{7.3.1\040Ideas}{section.7.3}% 122
\BOOKMARK [1][-]{section.7.4}{7.4\040ML\040questions}{chapter.7}% 123
